{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMPRT/YnSLXmHJ+h7EZxQAy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SY-256/anomaly_detection/blob/main/notebook/chapter6_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 多変数のホテリング理論による異常検知\n",
        "\n",
        "多変数のホテリング理論による異常検知\n",
        "- A.変数選択\n",
        "- B.モデルの学習\n",
        "- C.推論"
      ],
      "metadata": {
        "id": "oRUC2xOD8K3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A.変数選択"
      ],
      "metadata": {
        "id": "YI__eSjd_vYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ヒストグラムによる正常と異常の分離性の確認\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/ghmagazine/python_anomaly_detection_book/refs/heads/main/notebooks/datasets/ch2_dataset_train.csv\")\n",
        "# \"label\"列を削除（数値型の列のみとする）\n",
        "df_val = df.drop(\"label\", axis=1)\n",
        "# 正常データのみ取り出す\n",
        "df_normal_val = df[df[\"label\"] == \"normal\"].drop(\"label\", axis=1)\n",
        "# 異常データのみ取り出す\n",
        "df_anomaly_val = df[df[\"label\"] == \"anomaly\"].drop(\"label\", axis=1)\n",
        "fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(20, 4))\n",
        "\n",
        "# すべての変数をループで走査\n",
        "for i, colname in enumerate(df_normal_val.columns):\n",
        "    # 正常データのヒストグラム描画\n",
        "    sns.histplot(data=df_normal_val[colname], bins=\"sturges\",\n",
        "                 color=\"#bbbbbb\", ax=ax[i],\n",
        "                 stat=\"density\", label=\"normal\")\n",
        "    # 異常データのヒストグラム描画\n",
        "    sns.histplot(data=df_anomaly_val[colname], bins=\"sturges\",\n",
        "                 color=\"#111111\", ax=ax[i],\n",
        "                 stat=\"density\", label=\"anomaly\")\n",
        "\n",
        "    # 凡例を追加\n",
        "    ax[i].legend()\n",
        "    # 変数名を図のタイトルとして追加\n",
        "    ax[i].set_title(colname, fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-GtTi6Va8pBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "変数`temp1`、`temp2`、`temp3`において正常と異常の分布がある程度分離しているもの対し、`temp4`と`temp5`は両者の分布に目に見える差がない。よって、本データの異常の検知に寄与する可能性が低く、モデルの入力する必要性は低いと判断できる。"
      ],
      "metadata": {
        "id": "B_-DRqze-LDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 散布図（pairplot）による相関の確認\n",
        "sns.pairplot(\n",
        "    data=df,\n",
        "    vars=[\"temp1\", \"temp2\", \"temp3\"],\n",
        "    hue=\"label\",\n",
        "    palette=[\"#999999\", \"#111111\"]\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zIC3CAIH-2Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ホテリング理論は変数間の相関に比較的強いアルゴリズムであるので、弱相関程度であれば変数を削除しなくても良い。（今回は便宜上モデルの判定を可視化し易くするために、入力変数は`temp1`と`temp2`の2変数に絞る。"
      ],
      "metadata": {
        "id": "ruU5TwCm_Jlo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B.モデルの学習"
      ],
      "metadata": {
        "id": "QGm12tre_uSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習フェーズでは、最尤推定による多次元正規分布パラメータの推定、および異常度の閾値算出を実施"
      ],
      "metadata": {
        "id": "2N98WE_-_sfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 多変数のホテリング理論による異常検知の実装例（学習）\n",
        "# N >> Mが成り立つ場合の学習の実装（カイ二乗分布）\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "##### 学習データの読み込みと前処理 #####\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/ghmagazine/python_anomaly_detection_book/refs/heads/main/notebooks/datasets/ch2_dataset_train.csv\")\n",
        "# \"temp2\"と\"temp1\"に欠損があるデータを削除\n",
        "df_dropna = df.dropna(subset=[\"temp1\", \"temp2\"], axis=0)\n",
        "# 正常データのみを抽出\n",
        "df_normal = df_dropna[df_dropna[\"label\"] == \"normal\"]\n",
        "# \"temp2\", \"temp1\"列のみ取り出してNumpyのndarray化し、学習データとする\n",
        "X_train = df_normal[[\"temp2\", \"temp1\"]].to_numpy()\n",
        "\n",
        "###### 学習ステップ1. 正常のモデルを作成 #####\n",
        "mu = np.mean(X_train, axis=0) # 標本平均ベクトルμを算出\n",
        "# 標本分散共分散行列Σを算出（転置と自由度ddofに注意）\n",
        "Sigma = np.cov(X_train.T, ddof=0)\n",
        "\n",
        "##### 学習ステップ2. 異常を表す指標（異常度）を定義する #####\n",
        "# 式のみの定義\n",
        "\n",
        "#### 学習ステップ3. 異常度に閾値を設ける #####\n",
        "TARGET_FP_RATE = 0.0027 # ターゲットとする誤報率（正規分布の3σ相当=0.0027）\n",
        "n_features = X_train.shape[1] # 変数の数M\n",
        "# 自由度（M）のカイ二乗分布の累積分布関数の逆関数から閾値を算出\n",
        "a_th = stats.chi2.ppf(1-TARGET_FP_RATE, df=n_features)\n",
        "\n",
        "##### 学習で求めたパラメータをすべて表示 #####\n",
        "print(f\"mu={mu}\")\n",
        "print(f\"Sigma={Sigma}\")\n",
        "print(f\"a_th={a_th}\")"
      ],
      "metadata": {
        "id": "EQ_pKlG3ABSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N >> Mが成り立たない場合の学習の実装（F分布 自由度(M, N - M)）\n",
        "##### 学習ステップ1. 正常のモデルを作成する #####\n",
        "mu = np.mean(X_train, axis=0) # 標本平均ベクトルμを算出\n",
        "# 標本分散共分散行列Σを算出\n",
        "Sigma = np.cov(X_train.T, ddof=0)\n",
        "\n",
        "##### 学習ステップ2. 異常を表す指標（異常度）を算出 #####\n",
        "# 式を定義するのみ\n",
        "\n",
        "##### 学習ステップ3. 異常度に閾値を設ける #####\n",
        "TARGET_FP_RATE = 0.0027 # ターゲットとする誤報率（正規分布の3σ相当=0.0027）\n",
        "sample_size = len(X_train) # サンプルサイズN\n",
        "n_features = X_train.shape[1] # 変数の数M\n",
        "# 自由度（M, N - M）のF分布の累積分布関数の逆関数から閾値を算出\n",
        "a_th = (sample_size+1)*n_features/(sample_size-n_features) \\\n",
        "* stats.f.ppf(1-TARGET_FP_RATE, dfn=n_features, dfd=sample_size-n_features)\n",
        "\n",
        "##### 学習で求めたパラメータを表示 #####\n",
        "print(f\"mu={mu}\")\n",
        "print(f\"Sigma={Sigma}\")\n",
        "print(f\"a_th={a_th}\")"
      ],
      "metadata": {
        "id": "V6_0HcGnCF56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "カイ二乗分布を使用した場合と比較して、異常度の閾値がやや大きくなっており、誤報率を減らす方向（見逃し寄り）に閾値が設定されてることがわかる（右側より）"
      ],
      "metadata": {
        "id": "WRj9jtkbDnsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 推論\n",
        "\n",
        "学習フェーズで求めたパラメータと閾値を用いて、推論データに対する異常度の算出と異常判定を行う。なお、異常度の分布にカイ二乗分布を用いる（$N ≫ M$が成り立つ）場合もF分布を用いる（$N \\gg M$が成り立たない）場合も、推論の実装方法に変化はない。"
      ],
      "metadata": {
        "id": "at6_p2qsD96u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 多変数のホテリング理論による異常検知の実装例（推論）\n",
        "\n",
        "##### 学習したパラメータをここに記載 #####\n",
        "MU = [350.23771153, 400.21552883] # 標本平均ベクトルμ\n",
        "SIGMA = [[26.04088919, 14.52874949],\n",
        "         [14.52874949, 24.58401158]] # 標本分散共分散行列Σ\n",
        "A_TH=11.82900701194368 # 異常度のしきい値\n",
        "\n",
        "##### 推論データの読み込みと前処理 #####\n",
        "df_inference = pd.read_csv(\"https://raw.githubusercontent.com/ghmagazine/python_anomaly_detection_book/refs/heads/main/notebooks/datasets/ch2_dataset_inference.csv\")\n",
        "# \"temp2\"、\"temp1\"変数に欠損があるデータを削除\n",
        "df_inference_dropna = df_inference.dropna(subset=[\"temp2\", \"temp1\"])\n",
        "# \"temp2\"、\"temp1\"列のみ取り出してNumpyのndarray化し、推論データとする\n",
        "X_inference = df_inference_dropna[[\"temp2\", \"temp1\"]].to_numpy()\n",
        "# 標本分散共分散行列の逆行列\n",
        "Sigma_inv = np.linalg.inv(np.array(SIGMA))\n",
        "\n",
        "##### 推論を実行 #####\n",
        "# 異常度を算出\n",
        "X_dev = (X_inference-np.array(MU)).T # 推論データと平均ベクトルとの差 (x-μ)\n",
        "Dev_Sigma = X_dev.T @ Sigma_inv # (x-μ)^T * Σ^-1\n",
        "anomaly_scores = np.sum(np.multiply(Dev_Sigma, X_dev.T), axis=1)\n",
        "# 閾値により異常の有無を判定\n",
        "pred = np.where(anomaly_scores > A_TH, \"anomaly\", \"normal\")\n",
        "# 推論結果を表示\n",
        "print(pred)"
      ],
      "metadata": {
        "id": "VqAsarVfdNvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "推論結果の決定境界（異常と正常の判定の境界）を可視化"
      ],
      "metadata": {
        "id": "CAWlXRxMe1AT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 推論結果の決定境界を可視化\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import seaborn as sns\n",
        "\n",
        "# 異常度と異常判定をDataFrameに列として追加\n",
        "df_inference_dropna[\"anomaly_score\"] = anomaly_scores\n",
        "df_inference_dropna[\"prediction\"] = pred\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n",
        "\n",
        "##### 正常と異常の範囲の色分け ######\n",
        "# (x1,x2)格子点を作成（'temp2'をx1としている）\n",
        "x1_grid = np.linspace(df_inference_dropna[\"temp2\"].min()-5,\n",
        "                      df_inference_dropna[\"temp2\"].max()+5, 200)\n",
        "x2_grid = np.linspace(df_inference_dropna[\"temp1\"].min()-5,\n",
        "                      df_inference_dropna[\"temp1\"].max()+5, 200)\n",
        "X1, X2 = np.meshgrid(x1_grid, x2_grid)\n",
        "X_grid = np.c_[X1.ravel(), X2.ravel()]\n",
        "# 異常度を算出\n",
        "X_dev_grid = (X_grid-np.array(MU)).T # 推論データと平均ベクトルとの差（x-μ）\n",
        "Dev_Sigma_grid = X_dev_grid.T @ Sigma_inv # (x-μ)^T * Σ^-1\n",
        "anomaly_scores_grid = np.sum(np.multiply(Dev_Sigma_grid, X_dev_grid.T), axis=1)\n",
        "# 閾値判定\n",
        "pred_grid = np.where(anomaly_scores_grid > A_TH, 0, 1)\n",
        "# 正常と異常の境界をプロット\n",
        "pred_pivot = pred_grid.reshape(X1.shape)\n",
        "ax.contourf(X1, X2, pred_pivot,\n",
        "            cmap=cm.gray, alpha=0.5)\n",
        "\n",
        "##### 各データを散布図としてプロット #####\n",
        "sns.scatterplot(data=df_inference_dropna, x=\"temp2\", y=\"temp1\",\n",
        "                hue=\"label\", palette=[\"#999999\", \"#111111\"], ax=ax)\n",
        "# 凡例を追加\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6K91Y-vGfAQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# カイ二乗分布のパラメータ推定の実装\n",
        "# 自由度kの値を学習データの異常度から求める\n",
        "###### 学習ステップ3: 異常度に閾値を設ける ######\n",
        "TARGET_FP_RATE = 0.0027 # ターゲットとする誤報率（正規分布の3σ相当=0.0027）\n",
        "# 学習データから異常度を算出\n",
        "Sigma_inv = np.linalg.inv(Sigma) # 標本分散共分散行列の逆行列\n",
        "X_dev = (X_train - mu).T # 推論データと平均ベクトルの差\n",
        "Dev_Sigma = X_dev.T @ Sigma_inv # (X - μ)^T * Σ^-1\n",
        "anomaly_scores_train = np.sum(np.multiply(Dev_Sigma, X_dev.T), axis=1)\n",
        "# カイ二乗分布の自由度dfとスケールパラメータscaleを異常度の分布から最尤推定する\n",
        "params = stats.chi2.fit(anomaly_scores_train, floc=0)\n",
        "df = params[0]\n",
        "scale = params[2]\n",
        "# 自由度df, スケールパラメータscaleを持つ\n",
        "# カイ二乗分布の累積分布関数の逆関数から閾値を算出\n",
        "# スケールパラメータも最尤推定の対象にするのがポイント（前項では固定していた）\n",
        "a_th = stats.chi2.ppf(1-TARGET_FP_RATE, df=df, scale=scale)"
      ],
      "metadata": {
        "id": "kZpOOJ1DhK8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a_th)"
      ],
      "metadata": {
        "id": "DEgCSvMQPJAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# マハラノビス・タグチ法による異常検知\n",
        "\n",
        "- 変数ごとの寄与を評価する仕組みを導入した手法\n",
        "\n",
        "### マハラノビス・タグチ法による異常検知の概要\n",
        "1. マハラノビス距離の閾値判定で異常を検知する\n",
        "2. 変数ごとの異常への寄与度（SN比）を求める"
      ],
      "metadata": {
        "id": "7Xl5FYrGPKYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# パラメータ学習\n",
        "# 多変数のホテリング理論による異常検知の実装例（学習）\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "###### 学習データの読み込みと前処理 ######\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/ghmagazine/python_anomaly_detection_book/refs/heads/main/notebooks/datasets/ch2_dataset_train.csv\")\n",
        "# \"temp2\", \"temp1\"変数に欠損がある行を削除\n",
        "df_dropna = df.dropna(subset=[\"temp2\", \"temp1\"])\n",
        "# 正常データのみ抽出\n",
        "df_normal = df_dropna[df_dropna[\"label\"] == \"normal\"]\n",
        "# \"temp2\", \"temp1\"列のみ取り出してNumpyのndarray化し、学習データとする\n",
        "X_train = df_normal[[\"temp2\", \"temp1\"]].to_numpy()\n",
        "\n",
        "###### 学習ステップ1. 正常のモデルを作成する ######\n",
        "mu = np.mean(X_train, axis=0) # 標本平均ベクトルμを算出\n",
        "# 標本分散共分散行Σを算出（転置と自由度ddofに注意）\n",
        "Sigma = np.cov(X_train.T, ddof=0)\n",
        "\n",
        "###### 学習ステップ2. 異常を表す指標（異常度）を定義する ######\n",
        "# 式のみで定義\n",
        "\n",
        "###### 学習ステップ3. 異常度に閾値を設ける ######\n",
        "TARGET_FP_RATE = 0.0027\n",
        "n_features = X_train.shape[1] # 変数の数M\n",
        "# 自由度（M）のカイ二乗分布の累積分布関数の逆関数から閾値算出\n",
        "a_th = stats.chi2.ppf(1-TARGET_FP_RATE, df=n_features)\n",
        "\n",
        "###### 学習で求めたパラメータをすべて表示 ######\n",
        "print(f\"mu={mu}\")\n",
        "print(f\"Sigma={Sigma}\")\n",
        "print(f\"a_th={a_th}\")"
      ],
      "metadata": {
        "id": "qJoo5-01Pj9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 推論\n",
        "# マハラノビス・タグチ法による異常検知の実装例（推論）\n",
        "\n",
        "###### 学習したパラメータを記載 ######\n",
        "MU = [350.23771153, 400.21552883] # 標本平均ベクトルμ\n",
        "SIGMA = [[26.04088919, 14.52874949],\n",
        "         [14.52874949, 24.58401158]] # 標本分散共分散行列Σ\n",
        "A_TH=11.82900701194368 # 異常度のしきい値\n",
        "\n",
        "###### 推論データの読み込みと前処理 ######\n",
        "df_inference = pd.read_csv(\"https://raw.githubusercontent.com/ghmagazine/python_anomaly_detection_book/refs/heads/main/notebooks/datasets/ch2_dataset_inference.csv\")\n",
        "# \"temp2\", \"temp1\"に欠損があるデータを削除\n",
        "df_inference_dropna = df_inference.dropna(subset=[\"temp2\", \"temp1\"])\n",
        "# \"temp2\", \"temp1\"列のみ取り出してNumpyのndarray化し、推論データとする\n",
        "X_inference = df_inference_dropna[[\"temp2\", \"temp1\"]].to_numpy()\n",
        "# 標本分散共分散行列Σの逆行列\n",
        "Sigma_inv = np.linalg.inv(np.array(SIGMA))\n",
        "\n",
        "###### 推論フェーズ1. マハラノビス距離の閾値判定で異常を検知（ホテリング理論と同じ） ######\n",
        "# 異常度を算出\n",
        "X_dev = (X_inference-np.array(MU)).T # 推論データと平均ベクトルとの差（x-μ）\n",
        "Dev_Sigma = X_dev.T @ Sigma_inv # (x-μ)^T * Σ^-1\n",
        "anomaly_scores = np.sum(np.multiply(Dev_Sigma, X_dev.T), axis=1)\n",
        "# 閾値により異常の有無を判定\n",
        "pred = np.where(anomaly_scores > A_TH, \"anomaly\", \"normal\")\n",
        "# 推論結果を表示\n",
        "print(pred)\n",
        "\n",
        "###### 推論フェーズ2. 異常データに対して変数ごとのSN比を算出 ######\n",
        "# 異常データのみを抽出\n",
        "df_inference_dropna[\"prediction\"] = pred\n",
        "df_pred_anomaly = df_inference_dropna[df_inference_dropna[\"prediction\"] == \"anomaly\"]\n",
        "# 各変数の分散（標本分散共分散行列Σの対角成分）\n",
        "var = np.diag(np.array(SIGMA))\n",
        "# 変数ごとにSN比を計算\n",
        "for j, feature in enumerate([\"temp2\", \"temp1\"]):\n",
        "    sn_ratio = 10 * np.log10((df_pred_anomaly[feature]-MU[j]) ** 2 / var[j])\n",
        "    df_pred_anomaly[f\"sn_{feature}\"] = sn_ratio\n",
        "# 計算したSN比を含む異常データを表示\n",
        "print(df_pred_anomaly)"
      ],
      "metadata": {
        "id": "qPhq6YLXSo6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "異常判定されたデータに対してSN比を算出してみる"
      ],
      "metadata": {
        "id": "9gMy-2jlVWbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 異常判定されたデータのSN比を可視化\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import seaborn as sns\n",
        "\n",
        "# 異常度と異常判定をDataFrameに列として追加\n",
        "df_inference_dropna[\"anomaly_score\"] = anomaly_scores\n",
        "df_inference_dropna[\"prediction\"] = pred\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n",
        "\n",
        "###### 正常と異常の範囲の色分け #####\n",
        "# (x1, x2)格子点を作成('temp2'をx1としている点に注意)\n",
        "x1_grid = np.linspace(df_inference_dropna[\"temp2\"].min()-5,\n",
        "                      df_inference_dropna[\"temp2\"].max()+5, 100)\n",
        "x2_grid = np.linspace(df_inference_dropna[\"temp1\"].min()-5,\n",
        "                      df_inference_dropna[\"temp1\"].max()+5, 100)\n",
        "X1, X2 = np.meshgrid(x1_grid, x2_grid)\n",
        "X_grid = np.c_[X1.ravel(), X2.ravel()]\n",
        "# 異常度を算出\n",
        "X_dev_grid = (X_grid-np.array(MU)).T # 推論データと平均ベクトルとの差（x-μ）\n",
        "Dev_Sigma_grid = X_dev_grid.T @ Sigma_inv # (x-μ)^T * Σ^-1\n",
        "anomaly_score_grid = np.sum(np.multiply(Dev_Sigma_grid, X_dev_grid.T), axis=1)\n",
        "# 閾値判定\n",
        "pred_grid = np.where(anomaly_score_grid > A_TH, 0, 1)\n",
        "# 正常と異常の境界をプロット\n",
        "pred_pivot = pred_grid.reshape(X1.shape)\n",
        "ax.contourf(X1, X2, pred_pivot,\n",
        "            cmap=cm.gray, alpha=0.5)\n",
        "\n",
        "\n",
        "##### 各データを散布図としてプロット #####\n",
        "sns.scatterplot(data=df_inference_dropna, x=\"temp2\", y=\"temp1\",\n",
        "                hue=\"label\", palette=[\"#999999\", \"#111111\"], ax=ax)\n",
        "\n",
        "# 異常判定されたデータのみインデックスを表示\n",
        "for i, row in df_pred_anomaly.iterrows():\n",
        "    ax.text(row[\"temp2\"], row[\"temp1\"], str(i),\n",
        "            verticalalignment=\"bottom\", horizontalalignment=\"center\")\n",
        "# 凡例を表示\n",
        "ax.legend()\n",
        "# グラフを表示\n",
        "plt.show()\n",
        "\n",
        "###### 異常判定されたデータのSN比を表示 ######\n",
        "for i, row in df_pred_anomaly.iterrows():\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(3, 4))\n",
        "    # SN比を描画\n",
        "    sn_names = [f\"sn_{feature}\" for feature in [\"temp2\", \"temp1\"]]\n",
        "    sn_ratio = [row[sn_name] for sn_name in sn_names]\n",
        "    axes.bar(sn_names, sn_ratio, color=\"#888888\")\n",
        "    axes.set_title(f\"index={i}\")\n",
        "    plt.xticks(fontsize=12)\n",
        "    # グラフを表示\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jxAnR1jb0RfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "各データについて、どの変数が異常に強く寄与していたかを定量的に把握することができる。\n",
        "特に、正常ラベルでありながら異常と判定された誤報データ（`index=142`）では、他の異常データとSN比の傾向が異なることが確認できる。これは、両者の特徴空間での方向性が異なるためであり、このようなSN比の傾向を分析することが、誤報や未知の異常パターンの判別に役立つ。\n",
        "このような特徴空間での方向性は、2変数であらば散布図でも把握可能ですが、3変数以上になると可視化困難になる。そのため、SN比の活用価値は高次元データにおいて特に高まる。"
      ],
      "metadata": {
        "id": "0JSS2LZC3reY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fw5HiGHe5e4a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}